{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: ML-Based Extraction of Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "pdf_name = ''\n",
    "df = data_representation(pdf_name,df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = ycord_average(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = feature_engineering(df)\n",
    "df[ort_col+paper_feature+new_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create word embeddings\n",
    "df = word_embedding(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create sliding window for features and word embeddings\n",
    "df = sliding_window(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predict_labels(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports/Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.svm import LinearSVC #SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# Other Packages\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "# From PDFInterpreter import both PDFResourceManager and PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "# Import this to raise exception whenever text extraction from PDF is not allowed\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine, LTTextBoxHorizontal, LTFigure, LTChar, LTText, LTAnno\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.externals import joblib\n",
    "import random\n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import pickle\n",
    "import joblib\n",
    "import json\n",
    "from gensim.models import KeyedVectors\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define empty lists for columns\n",
    "pages = []\n",
    "xcord = []\n",
    "ycord = []\n",
    "xcords_first = []\n",
    "ycords_first = []\n",
    "font_names = []\n",
    "content = []\n",
    "docs = []\n",
    "objects = []\n",
    "textboxes = []\n",
    "i = 0\n",
    "words = []\n",
    "\n",
    "password = \"\"\n",
    "extracted_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_col = ['doc', 'Page', 'word']\n",
    "paper_feature = ['word.is.lower', 'word.is.upper', 'word.is.mixed.case', 'word.is.digit', 'word.contains.digit', 'word.is.special.char','word.len.1', 'word.len.3', 'word.len.5', 'word.len.7', 'word.len.9', 'word.len.11', 'word.len.13', 'word.is.stop']\n",
    "date_specific_feature = ['word.is.print.date.trigger', 'word.is.revision.date.trigger', 'word.is.valid.date.trigger', 'word.is.oldversion.date.trigger']\n",
    "new_feature = ['word.is.bold', 'word.is.newline','ycord_average','Xcord_first', 'grid.area_11', 'grid.area_12', 'grid.area_13', 'grid.area_14', 'grid.area_15', 'grid.area_16', 'grid.area_17', 'grid.area_18', 'grid.area_21', 'grid.area_22', 'grid.area_23', 'grid.area_24', 'grid.area_25', 'grid.area_26', 'grid.area_27', 'grid.area_28', 'grid.area_31', 'grid.area_32', 'grid.area_33', 'grid.area_34', 'grid.area_35', 'grid.area_36', 'grid.area_37', 'grid.area_38', 'grid.area_41', 'grid.area_42', 'grid.area_43', 'grid.area_44', 'grid.area_45', 'grid.area_46', 'grid.area_47', 'grid.area_48', 'is.page.1', 'is.page.2', 'is.page.3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data Representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for checking if token contains a number\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_words(lt_objs, pagenum, doc):\n",
    "        objectnum = 1\n",
    "        #iterate through Textboxes\n",
    "        for obj in lt_objs:\n",
    "                textboxnum = 1\n",
    "                if isinstance(obj, LTTextBoxHorizontal):\n",
    "                #iterate through TextLines\n",
    "                        for o in obj._objs:\n",
    "                                if isinstance(o, LTTextLine):\n",
    "                                        text=o.get_text()\n",
    "                                        #print(text)\n",
    "                                        if text.strip():\n",
    "                                                word = ''\n",
    "                                                #iterate through characters\n",
    "                                                for c in  o._objs:\n",
    "                                                        #if it is a character\n",
    "                                                        if isinstance(c, LTChar):\n",
    "                                                                #detect special character at end of words\n",
    "                                                                if c.get_text() in (')',':','%',';','('):\n",
    "                                                                    words.extend([word, str(c.get_text())])\n",
    "                                                                    pages.extend([pagenum, pagenum])\n",
    "                                                                    xcords_first.extend([xcord_first,c.bbox[0]])\n",
    "                                                                    ycords_first.extend([ycord_first,c.bbox[1]])\n",
    "                                                                    docs.extend([doc, doc])\n",
    "                                                                    font_names.extend([fontname,c.fontname])\n",
    "                                                                    objects.extend([objectnum, objectnum])\n",
    "                                                                    textboxes.extend([textboxnum, textboxnum])\n",
    "                                                                    word = ''\n",
    "                                                                # if / in between of numbers --> not a new token\n",
    "                                                                elif c.get_text() in ('/') and not word.isdigit() and not word == 'g':\n",
    "                                                                    words.extend([word, str(c.get_text())])\n",
    "                                                                    pages.extend([pagenum, pagenum])\n",
    "                                                                    xcords_first.extend([xcord_first,c.bbox[0]])\n",
    "                                                                    ycords_first.extend([ycord_first,c.bbox[1]])\n",
    "                                                                    docs.extend([doc, doc])\n",
    "                                                                    font_names.extend([fontname,c.fontname])\n",
    "                                                                    objects.extend([objectnum, objectnum])\n",
    "                                                                    textboxes.extend([textboxnum, textboxnum])\n",
    "                                                                    word = ''\n",
    "                                                                # if , in between of numbers --> not a new token\n",
    "                                                                elif c.get_text() in (',') and not word.isdigit():\n",
    "                                                                    words.extend([word, str(c.get_text())])\n",
    "                                                                    pages.extend([pagenum, pagenum])\n",
    "                                                                    xcords_first.extend([xcord_first,c.bbox[0]])\n",
    "                                                                    ycords_first.extend([ycord_first,c.bbox[1]])\n",
    "                                                                    font_names.extend([fontname,c.fontname])\n",
    "                                                                    docs.extend([doc, doc])\n",
    "                                                                    objects.extend([objectnum, objectnum])\n",
    "                                                                    textboxes.extend([textboxnum, textboxnum])\n",
    "                                                                    word = ''\n",
    "                                                                \n",
    "                                                                # take 'n.v' as an exception and check that . is not in between numbers or dates\n",
    "                                                                elif c.get_text() in ('.') and not hasNumbers(word) and not word == 'n' and '@' not in word:\n",
    "                                                                    words.extend([word, str(c.get_text())])\n",
    "                                                                    pages.extend([pagenum, pagenum])\n",
    "                                                                    xcords_first.extend([xcord_first,c.bbox[0]])\n",
    "                                                                    ycords_first.extend([ycord_first,c.bbox[1]])\n",
    "                                                                    font_names.extend([fontname,c.fontname])\n",
    "                                                                    docs.extend([doc, doc])\n",
    "                                                                    objects.extend([objectnum, objectnum])\n",
    "                                                                    textboxes.extend([textboxnum, textboxnum])\n",
    "                                                                    word = ''\n",
    "                                                                    \n",
    "                                                                else:\n",
    "                                                                    #append until space\n",
    "                                                                    word += str(c.get_text())\n",
    "                                                                    #remember coords if its first char of word\n",
    "                                                                    if len(word) == 1:\n",
    "                                                                            xcord_first = c.bbox[0]\n",
    "                                                                            ycord_first = c.bbox[1]\n",
    "                                                                            fontname = c.fontname\n",
    "                                                                    # if space and previous token was not space: append word to list (without the space) and start new word\n",
    "                                                                    if c.get_text() == ' ':\n",
    "                                                                            words.append(word[:-1])\n",
    "                                                                            pages.append(pagenum)\n",
    "                                                                            xcords_first.append(xcord_first)\n",
    "                                                                            ycords_first.append(ycord_first)\n",
    "                                                                            font_names.append(fontname)\n",
    "                                                                            docs.append(doc)\n",
    "                                                                            objects.append(objectnum)\n",
    "                                                                            textboxes.append(textboxnum)\n",
    "                                                                            word = ''\n",
    "                                                        #if it is a new line and word is not empty: append word to list and start new word\n",
    "                                                        if isinstance(c, LTAnno):\n",
    "                                                                words.append(word)\n",
    "                                                                pages.append(pagenum)\n",
    "                                                                xcords_first.append(xcord_first)\n",
    "                                                                ycords_first.append(ycord_first)\n",
    "                                                                font_names.append(fontname)\n",
    "                                                                docs.append(doc)\n",
    "                                                                objects.append(objectnum)\n",
    "                                                                textboxes.append(textboxnum)\n",
    "                                                                word = ''\n",
    "             \n",
    "                                textboxnum = textboxnum + 1\n",
    "                                                                                         \n",
    "                # if it's a container, recurse\n",
    "                elif isinstance(obj, LTFigure):\n",
    "                        parse_words(obj._objs, pagenum, doc)\n",
    "\n",
    "                objectnum = objectnum + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_representation(pdf_name, df):\n",
    "    \n",
    "    #read PDF file\n",
    "    doc = pdf_name\n",
    "    fp = open(doc, 'rb')\n",
    "\n",
    "    # Create parser object to parse the pdf content\n",
    "    parser = PDFParser(fp)\n",
    "\n",
    "    # Store the parsed content in PDFDocument object\n",
    "    document = PDFDocument(parser, password)\n",
    "\n",
    "    # Create PDFResourceManager object that stores shared resources such as fonts or images\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "\n",
    "    # set parameters for analysis\n",
    "    laparams = LAParams()\n",
    "\n",
    "    # Create a PDFDevice object which translates interpreted information into desired format\n",
    "    # Device needs to be connected to resource manager to store shared resources\n",
    "    device = PDFDevice(rsrcmgr)\n",
    "    # Extract the decive to page aggregator to get LT object elements\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "    # Create interpreter object to process page content from PDFDocument\n",
    "    # Interpreter needs to be connected to resource manager for shared resources and device \n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # process only first 3 pages \n",
    "    for p ,page in enumerate(PDFPage.create_pages(document)):\n",
    "        pagenum = p + 1        \n",
    "        if pagenum == 4:\n",
    "            break\n",
    "        # As the interpreter processes the page stored in PDFDocument object\n",
    "        interpreter.process_page(page)\n",
    "        # The device renders the layout from interpreter\n",
    "        layout = device.get_result()\n",
    "\n",
    "        # call function to parse on character level and identify tokens\n",
    "        parse_words(layout._objs, pagenum, doc)\n",
    "\n",
    "\n",
    "    #close the pdf file\n",
    "    fp.close()\n",
    "    \n",
    "    #save data in dataframe\n",
    "    \n",
    "    #creat empty dataframe\n",
    "    df = pd.DataFrame( \n",
    "        {\n",
    "         'doc': docs,\n",
    "         'Page': pages,\n",
    "         'Ycord_first': ycords_first,\n",
    "         'Xcord_first': xcords_first,\n",
    "         'font_name': font_names,\n",
    "         'Object': objects,\n",
    "         'Textbox': textboxes,\n",
    "         'word': words\n",
    "        })\n",
    "\n",
    "    df = df.sort_values(['doc','Page','Ycord_first','Xcord_first'],ascending=[True,True,False,True])\n",
    "\n",
    "    #delete empty tokens\n",
    "    df = df[ df[\"word\"] != \"\"]\n",
    "\n",
    "    #identify special characters\n",
    "    df['special_char'] = df['word'].apply(lambda x: 1 if (x in ('\"*\"', 'ꞏ') or (len(x) == 1 and x.isalnum () == False)) else 0 )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by average of y-coordinate within the same line\n",
    "\n",
    "def ycord_average(df):\n",
    "\n",
    "    yc = df['Ycord_first']\n",
    "    yc_new = []\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    first = True\n",
    "    for index, i in enumerate(yc):\n",
    "        #very first row\n",
    "        if first == True:\n",
    "            avg = i\n",
    "            counter += 1 \n",
    "        elif abs(avg - i) <= 5:\n",
    "            avg = avg - ((avg - i)/(counter+1))\n",
    "            counter += 1\n",
    "            #very last row\n",
    "            if index == (len(yc)-1):\n",
    "                yc_new += counter * [avg]\n",
    "        elif abs(avg - i) > 5:\n",
    "            yc_new += counter * [avg]\n",
    "            avg = i\n",
    "            counter = 1\n",
    "\n",
    "\n",
    "        first = False\n",
    "    \n",
    "    #yc_new.append(yc_new[-1])\n",
    "\n",
    "    df['ycord_average'] = yc_new\n",
    "\n",
    "\n",
    "    df = df.sort_values(['doc','Page','ycord_average','Xcord_first'],ascending=[True,True,False,True])\n",
    "\n",
    "    df.reset_index(inplace=True, drop = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "\n",
    "    words = list(df['word'])\n",
    "\n",
    "    #set german stopwords\n",
    "    stopWords = set(stopwords.words('german'))\n",
    "\n",
    "    # define own special character set \n",
    "    special_chars = ('.',',','(', ')', '–', '[', '·','{', '}', ']', ':', ';', \"'\", '\"','?', '/', '*','!', '@', '#', '&', '\"*\"', '`', '~', '$', '^', '+', '=', '<', '>','%')\n",
    "\n",
    "    #prepare ranges for x-y-coordinate grid classification\n",
    "    num_rows = 8\n",
    "    num_cols = 4\n",
    "\n",
    "    x_width = int(max(df['Xcord_first'])) / num_cols\n",
    "    y_width = int(max(df['ycord_average'])) / num_rows\n",
    "\n",
    "\n",
    "    features = {\n",
    "\n",
    "    # meta information: style of characters\n",
    "\n",
    "        'word.is.lower': [],\n",
    "        'word.is.upper': [],\n",
    "        'word.is.mixed.case': [],\n",
    "        'word.is.bold':[],\n",
    "\n",
    "\n",
    "    # meta information: type of characters\n",
    "\n",
    "        'word.is.digit': [],\n",
    "        'word.contains.digit':[],\n",
    "        'word.is.special.char' :[],\n",
    "\n",
    "\n",
    "    # length of words\n",
    "\n",
    "        'word.len.1':[],\n",
    "        'word.len.3':[],\n",
    "        'word.len.5':[],\n",
    "        'word.len.7':[],\n",
    "        'word.len.9':[],\n",
    "        'word.len.11':[],\n",
    "        'word.len.13':[],\n",
    "\n",
    "    # Semantic\n",
    "\n",
    "        #'word.pos':[],\n",
    "        'word.is.stop':[],\n",
    "        'word.is.print.date.trigger':[],\n",
    "        'word.is.revision.date.trigger':[],\n",
    "        'word.is.oldversion.date.trigger':[],\n",
    "        'word.is.valid.date.trigger':[],\n",
    "\n",
    "\n",
    "        # Graphical Information / Location of word(token)\n",
    "        'grid.area':[],\n",
    "\n",
    "        'is.page.1':[],\n",
    "        'is.page.2':[],\n",
    "        'is.page.3':[],\n",
    "\n",
    "        'word.is.newline': []    \n",
    "        }\n",
    "\n",
    "    '''\n",
    "    # Graphical Information / Location of word(token)\n",
    "\n",
    "\n",
    "        'word.is.koord.1':[],\n",
    "        'word.is.koord.2':[],\n",
    "        'word.is.koord.3':[],\n",
    "        'word.is.koord.4':[],\n",
    "        'word.is.koord.5':[],\n",
    "        'word.is.koord.6':[],\n",
    "        'word.is.koord.7':[],\n",
    "        'word.is.koord.8':[],\n",
    "\n",
    "        'is.page.1':[],\n",
    "        'is.page.2':[],\n",
    "        'is.page.3':[],\n",
    "\n",
    "        'word.is.newline': []    \n",
    "    '''\n",
    "\n",
    "\n",
    "    for i, w in enumerate(words):\n",
    "\n",
    "        '''\n",
    "        meta information: style of characters\n",
    "        '''\n",
    "        features['word.is.lower'].append(str(w).islower())\n",
    "        features['word.is.upper'].append(str(w).isupper())\n",
    "\n",
    "        if not str(w).islower() and not str(w).isupper():\n",
    "            features['word.is.mixed.case'].append(1)\n",
    "        else:\n",
    "            features['word.is.mixed.case'].append(0)\n",
    "\n",
    "        if 'bold' in str(df.loc[i,'font_name']).lower():\n",
    "            features['word.is.bold'].append(1)\n",
    "        else:\n",
    "            features['word.is.bold'].append(0)\n",
    "\n",
    "\n",
    "        '''\n",
    "        meta information: type of characters\n",
    "        '''\n",
    "\n",
    "        features['word.is.digit'].append(str(w).isdigit())\n",
    "        if any(x.isdigit() for x in str(w)):\n",
    "            features['word.contains.digit'].append(1)\n",
    "        else:\n",
    "            features['word.contains.digit'].append(0)\n",
    "\n",
    "        if str(w) in special_chars:\n",
    "            features['word.is.special.char'].append(1)\n",
    "        else:\n",
    "            features['word.is.special.char'].append(0)\n",
    "            \n",
    "\n",
    "        '''\n",
    "        length of words\n",
    "        '''\n",
    "        if len(str(w)) > 12:\n",
    "            features['word.len.13'].append(1)\n",
    "            for l in (1,3,5,7,9,11):\n",
    "                features['word.len.' + str(l)].append(0)\n",
    "        elif len(str(w)) > 10:\n",
    "            features['word.len.11'].append(1)\n",
    "            for l in (1,3,5,7,9,13):\n",
    "                features['word.len.' + str(l)].append(0)\n",
    "        elif len(str(w)) > 8:\n",
    "            features['word.len.9'].append(1)\n",
    "            for l in (1,3,5,7,11,13):\n",
    "                features['word.len.' + str(l)].append(0)\n",
    "        elif len(str(w)) > 6:\n",
    "            features['word.len.7'].append(1)\n",
    "            for l in (1,3,5,9,11,13):\n",
    "                features['word.len.' + str(l)].append(0)\n",
    "        elif len(str(w)) > 4:\n",
    "            features['word.len.5'].append(1)\n",
    "            for l in (1,3,7,9,11,13):\n",
    "                features['word.len.' + str(l)].append(0)\n",
    "        elif len(str(w)) > 2:\n",
    "            features['word.len.3'].append(1)\n",
    "            for l in (1,5,7,9,11,13):\n",
    "                features['word.len.' + str(l)].append(0)\n",
    "        else:\n",
    "            features['word.len.1'].append(1)\n",
    "            for l in (3,5,7,9,11,13):\n",
    "                features['word.len.' + str(l)].append(0)\n",
    "\n",
    "        '''\n",
    "        Semantic\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "        if str(w) in stopWords:\n",
    "            features['word.is.stop'].append(1)\n",
    "        else:\n",
    "            features['word.is.stop'].append(0)\n",
    "\n",
    "        if str(w).lower() in ['druck', 'ausgabe', 'ausstellung', 'erstellung', 'sd-datum', 'erstellt', 'ausgestellt']:\n",
    "            features['word.is.print.date.trigger'].append(1)\n",
    "        else:\n",
    "            features['word.is.print.date.trigger'].append(0)\n",
    "\n",
    "        if str(w).lower() in ['überarbeit', 'änderung', 'revision', 'bearbeitung', 'quick-fds']:\n",
    "            features['word.is.revision.date.trigger'].append(1)\n",
    "        else:\n",
    "            features['word.is.revision.date.trigger'].append(0)\n",
    "\n",
    "        if str(w).lower() in  ['ersetzt', 'ersatz', 'fassung', 'letzten']:\n",
    "            features['word.is.oldversion.date.trigger'].append(1)\n",
    "        else:\n",
    "            features['word.is.oldversion.date.trigger'].append(0)\n",
    "\n",
    "        if str(w).lower() in ['kraft', 'freigabe']:\n",
    "            features['word.is.valid.date.trigger'].append(1)\n",
    "        else:\n",
    "            features['word.is.valid.date.trigger'].append(0)\n",
    "\n",
    "\n",
    "        # Graphical Information / Location of word(token)\n",
    "\n",
    "        for r in reversed(range(num_rows)):\n",
    "            if df.loc[i,'ycord_average'] > r*y_width:\n",
    "                y_cluster = r+1\n",
    "                break\n",
    "            else:\n",
    "                y_cluster = 1\n",
    "        for c in reversed(range(num_cols)):\n",
    "            if df.loc[i,'Xcord_first'] > c*x_width:\n",
    "                x_cluster = c+1\n",
    "                break\n",
    "            else:\n",
    "                x_cluster = 1\n",
    "\n",
    "        cluster = str(str(x_cluster) + str(y_cluster))\n",
    "        features['grid.area'].append(cluster)\n",
    "\n",
    "        if df.loc[i,'Page'] == 1:\n",
    "            features['is.page.1'].append(1)\n",
    "            features['is.page.2'].append(0)\n",
    "            features['is.page.3'].append(0)\n",
    "        elif df.loc[i,'Page'] == 2:\n",
    "            features['is.page.1'].append(0)\n",
    "            features['is.page.2'].append(1)\n",
    "            features['is.page.3'].append(0)\n",
    "        elif df.loc[i,'Page'] == 3:\n",
    "            features['is.page.1'].append(0)\n",
    "            features['is.page.2'].append(0)\n",
    "            features['is.page.3'].append(1)\n",
    "\n",
    "        if i > 1:\n",
    "            if df.loc[i,'ycord_average'] != df.loc[i-1,'ycord_average']:\n",
    "                features['word.is.newline'].append(1)\n",
    "            else:\n",
    "                features['word.is.newline'].append(0)\n",
    "        else:\n",
    "            features['word.is.newline'].append(1)\n",
    "\n",
    "\n",
    "    features = pd.DataFrame(features)\n",
    "\n",
    "    orientation_col = df[['doc', 'Page', 'word']]\n",
    "\n",
    "    df = pd.concat((orientation_col, features, df[['ycord_average', 'Xcord_first']]), axis=1, sort=False)\n",
    "    \n",
    "    #encode columns\n",
    "\n",
    "    #one-hot grid areas\n",
    "    df.loc [:,'grid.area'] = pd.Categorical(df['grid.area'])\n",
    "    grid_dummies = pd.get_dummies(df['grid.area'], prefix = 'grid.area') \n",
    "\n",
    "    #first columns as id, not part of the model \n",
    "    orientation_col = df.loc[:, 'doc':'word']\n",
    "\n",
    "    #select feature columns\n",
    "    features = df.loc[:, 'word.is.lower':'Xcord_first']\n",
    "    #change remaining features from True/False to 0/1\n",
    "    features.loc[:,'word.is.lower'] = features['word.is.lower'].astype(int)\n",
    "    features.loc[:,'word.is.upper'] = features['word.is.upper'].astype(int)\n",
    "    features.loc[:,'word.is.digit'] = features['word.is.digit'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.concat((orientation_col, grid_dummies, features), axis=1, sort=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word embeddings\n",
    "\n",
    "def word_embedding(df):\n",
    "\n",
    "    #preprocess words\n",
    "    word_emb_input = pd.DataFrame(df.loc[:,'word'].str.lower())\n",
    "    word_emb_input = pd.DataFrame(word_emb_input)\n",
    "    word_emb_input = word_emb_input.astype(str)\n",
    "    word_emb_input['preprocessed'] = np.nan\n",
    "    word_emb_input ['preprocessed'] = [re.sub('\\d', 'D', x) for x in word_emb_input['word'].tolist()]\n",
    "    word_emb_input = word_emb_input.drop(['word'],1)\n",
    "\n",
    "    #create embeddings\n",
    "\n",
    "    #calculate embeddings for every word\n",
    "    missing=[0]*300\n",
    "    def fun(key):\n",
    "        try:\n",
    "            return(en_model[key])\n",
    "        except:\n",
    "            return(missing)\n",
    "    word_emb_input['vector'] = word_emb_input['preprocessed'].apply(fun)\n",
    "    word_emb = pd.DataFrame(word_emb_input['vector'].values.tolist())\n",
    "\n",
    "    #concat data\n",
    "    emb_data = pd.concat((df, word_emb), axis=1, sort=False)\n",
    "    return emb_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(df):\n",
    "\n",
    "    # create sliding window\n",
    "\n",
    "    #Feature Groups\n",
    "    ort_col = ['doc', 'Page', 'word']\n",
    "    paper_feature = ['word.is.lower', 'word.is.upper', 'word.is.mixed.case', 'word.is.digit', 'word.contains.digit', 'word.is.special.char','word.len.1', 'word.len.3', 'word.len.5', 'word.len.7', 'word.len.9', 'word.len.11', 'word.len.13', 'word.is.stop']\n",
    "    date_specific_feature = ['word.is.print.date.trigger', 'word.is.revision.date.trigger', 'word.is.valid.date.trigger', 'word.is.oldversion.date.trigger']\n",
    "    new_feature = ['word.is.bold', 'word.is.newline','ycord_average','Xcord_first', 'grid.area_11', 'grid.area_12', 'grid.area_13', 'grid.area_14', 'grid.area_15', 'grid.area_16', 'grid.area_17', 'grid.area_18', 'grid.area_21', 'grid.area_22', 'grid.area_23', 'grid.area_24', 'grid.area_25', 'grid.area_26', 'grid.area_27', 'grid.area_28', 'grid.area_31', 'grid.area_32', 'grid.area_33', 'grid.area_34', 'grid.area_35', 'grid.area_36', 'grid.area_37', 'grid.area_38', 'grid.area_41', 'grid.area_42', 'grid.area_43', 'grid.area_44', 'grid.area_45', 'grid.area_46', 'grid.area_47', 'grid.area_48', 'is.page.1', 'is.page.2', 'is.page.3']\n",
    "\n",
    "    data_ord = df[ort_col]\n",
    "\n",
    "    columns = [paper_feature, date_specific_feature, new_feature]\n",
    "\n",
    "    #create window for all features except word embedding\n",
    "    window_size_feat = 13\n",
    "    #copies the previous and following features for every token in a given window\n",
    "    for col in range (len(columns)):\n",
    "        sel_col = df[columns[col]]\n",
    "        data_ord = pd.concat([data_ord, sel_col], axis=1, sort=False)\n",
    "        for i in range (1, math.ceil(window_size_feat/2)):\n",
    "            data_pre = sel_col.shift(i).add_prefix ('-' + str(i) + '_')\n",
    "            data_suc = sel_col.shift(-i).add_prefix ('+' + str(i) + '_')\n",
    "            data_ord = pd.concat([data_ord, data_pre, data_suc], axis=1, sort=False)\n",
    "\n",
    "    df.columns = df.columns.astype(str)\n",
    "    \n",
    "    emd_col = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
    "'23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65',\n",
    "'66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162',\n",
    "'163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291',\n",
    "'292', '293', '294', '295', '296', '297', '298','299']\n",
    "\n",
    "    \n",
    "    #create window for word embedding\n",
    "    window_size_emb = 3\n",
    "    sel_col = df.loc[:,emd_col]\n",
    "    data_ord = pd.concat([data_ord, sel_col], axis=1, sort=False)\n",
    "    for i in range (1, math.ceil(window_size_emb/2)):\n",
    "        data_pre = sel_col.shift(i).add_prefix ('-' + str(i) + '_')\n",
    "        data_suc = sel_col.shift(-i).add_prefix ('+' + str(i) + '_')\n",
    "        data_ord = pd.concat([data_ord, data_pre, data_suc], axis=1, sort=False)\n",
    "        \n",
    "    return data_ord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ML Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(df):\n",
    "    \n",
    "  \n",
    "    # load the model from disk\n",
    "    ml_model = joblib.load('Own+Paper+Wordembedding_label_group.pkl')\n",
    "\n",
    "    #prepare data\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    df1 = df.loc[:,'word.is.bold':'+6_is.page.2'] #'+6_is.page.3'\n",
    "    df2 = df.loc[:,'word.is.lower':'+6_word.len.13']#+6_word.is.stop\n",
    "    df3 = df.loc[:,'0':'+1_298']#+1_299\n",
    "\n",
    "    X = pd.concat([df1, df2, df3], axis=1, sort=False)\n",
    "    \n",
    "    #predict labels\n",
    "    y_pred = ml_model.predict(X)\n",
    "\n",
    "\n",
    "    df['label_pred'] = y_pred\n",
    "\n",
    "    df_pred = df.loc[:,['word','label_pred']]\n",
    "\n",
    "    version = df_pred[df_pred['label_pred'] == 8]\n",
    "    version = list(version['word'])\n",
    "\n",
    "    directive = df_pred[df_pred['label_pred'] == 3]\n",
    "    directive = list(directive['word'])\n",
    "\n",
    "    signal = df_pred[df_pred['label_pred'] == 4]\n",
    "    signal = list(signal['word'])\n",
    "\n",
    "    chapter = df_pred[df_pred['label_pred'] == 1]\n",
    "    chapter = list(chapter['word'])\n",
    "\n",
    "    subchapter = df_pred[df_pred['label_pred'] == 5]\n",
    "    subchapter = list(subchapter['word'])\n",
    "\n",
    "    usecase_con = df_pred[df_pred['label_pred'] == 6]\n",
    "    usecase_con = list(usecase_con['word'])\n",
    "\n",
    "    usecase_pro = df_pred[df_pred['label_pred'] == 7]\n",
    "    usecase_pro = list(usecase_pro['word'])\n",
    "\n",
    "    '''\n",
    "    dict = {\n",
    "        'directive': directive,\n",
    "        'signal': signal,\n",
    "        'version-nr': version,  \n",
    "        'chapter': chapter,\n",
    "        'subchapter': subchapter,\n",
    "        'should be used for': usecase_pro,\n",
    "        'should not be used for': usecase_con\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    print ('   ************************** Verordnung **************************\\n')\n",
    "    print(directive)\n",
    "    print('\\n')\n",
    "\n",
    "    print ('   ************************** Versionsnummer **************************\\n')\n",
    "    print(version)\n",
    "    print('\\n')\n",
    "\n",
    "    print ('   ************************** Chapter **************************\\n')\n",
    "    print(chapter)\n",
    "    print('\\n')\n",
    "\n",
    "    print ('   ************************** Subchapter **************************\\n')\n",
    "    print(subchapter)\n",
    "    print('\\n')\n",
    "\n",
    "    print ('   ************************** Signalwort **************************\\n')\n",
    "    print(signal)\n",
    "    print('\\n')\n",
    "\n",
    "    print ('   ************************** Relevante Verwendung **************************\\n')\n",
    "    print(usecase_pro)\n",
    "    print('\\n')\n",
    "\n",
    "    print ('   ************************** Keine Verwendung **************************\\n')\n",
    "    print(usecase_con)\n",
    "    print('\\n')\n",
    "\n",
    "    #print(json.dumps(dict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load existing embeddings\n",
    "en_model = KeyedVectors.load_word2vec_format('data/cc.de.300.vec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtbda_venv",
   "language": "python",
   "name": "rtbda_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
