{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.svm import LinearSVC #SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# Other Packages\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "# From PDFInterpreter import both PDFResourceManager and PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "# Import this to raise exception whenever text extraction from PDF is not allowed\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine, LTTextBoxHorizontal, LTFigure, LTChar, LTText, LTAnno\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.externals import joblib\n",
    "import random\n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import pickle\n",
    "import joblib\n",
    "import json\n",
    "from gensim.models import KeyedVectors\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load existing embeddings\n",
    "en_model = KeyedVectors.load_word2vec_format('cc.de.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data with features\n",
    "\n",
    "data =  pd.read_csv('03_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(df):\n",
    "\n",
    "    #preprocess words\n",
    "    word_emb_input = pd.DataFrame(df.loc[:,'word'].str.lower())\n",
    "    word_emb_input = pd.DataFrame(word_emb_input)\n",
    "    word_emb_input = word_emb_input.astype(str)\n",
    "    word_emb_input['preprocessed'] = np.nan\n",
    "    word_emb_input ['preprocessed'] = [re.sub('\\d', 'D', x) for x in word_emb_input['word'].tolist()]\n",
    "    word_emb_input = word_emb_input.drop(['word'],1)\n",
    "\n",
    "    #create embeddings\n",
    "\n",
    "    #calculate embeddings for every word\n",
    "    missing=[0]*300\n",
    "    def fun(key):\n",
    "        try:\n",
    "            return(en_model[key])\n",
    "        except:\n",
    "            return(missing)\n",
    "    word_emb_input['vector'] = word_emb_input['preprocessed'].apply(fun)\n",
    "    word_emb = pd.DataFrame(word_emb_input['vector'].values.tolist())\n",
    "\n",
    "    #concat data\n",
    "    emb_data = pd.concat((df, word_emb), axis=1, sort=False)\n",
    "    return emb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(df):\n",
    "\n",
    "    # create sliding window\n",
    "\n",
    "    #Feature Groups\n",
    "    ort_col = ['doc', 'Page', 'word']\n",
    "    labels = ['chapter', 'subchapter', 'version', 'directive', 'signal', 'chem', 'company', 'date','usecase']\n",
    "    paper_feature = ['word.is.lower', 'word.is.upper', 'word.is.mixed.case', 'word.is.digit', 'word.contains.digit', 'word.is.special.char','word.len.1', 'word.len.3', 'word.len.5', 'word.len.7', 'word.len.9', 'word.len.11', 'word.len.13', 'word.is.stop']\n",
    "    date_specific_feature = ['word.is.print.date.trigger', 'word.is.revision.date.trigger', 'word.is.valid.date.trigger', 'word.is.oldversion.date.trigger']\n",
    "    new_feature = ['word.is.bold', 'word.is.newline','ycord_average','Xcord_first', 'grid.area_11', 'grid.area_12', 'grid.area_13', 'grid.area_14', 'grid.area_15', 'grid.area_16', 'grid.area_17', 'grid.area_18', 'grid.area_21', 'grid.area_22', 'grid.area_23', 'grid.area_24', 'grid.area_25', 'grid.area_26', 'grid.area_27', 'grid.area_28', 'grid.area_31', 'grid.area_32', 'grid.area_33', 'grid.area_34', 'grid.area_35', 'grid.area_36', 'grid.area_37', 'grid.area_38', 'grid.area_41', 'grid.area_42', 'grid.area_43', 'grid.area_44', 'grid.area_45', 'grid.area_46', 'grid.area_47', 'grid.area_48', 'is.page.1', 'is.page.2', 'is.page.3']\n",
    "\n",
    "    data_ord = df[ort_col + labels]\n",
    "\n",
    "    columns = [paper_feature, date_specific_feature, new_feature]\n",
    "\n",
    "    #create window for all features except word embedding\n",
    "    window_size_feat = 13\n",
    "    #copies the previous and following features for every token in a given window\n",
    "    for col in range (len(columns)):\n",
    "        sel_col = df[columns[col]]\n",
    "        data_ord = pd.concat([data_ord, sel_col], axis=1, sort=False)\n",
    "        for i in range (1, math.ceil(window_size_feat/2)):\n",
    "            data_pre = sel_col.shift(i).add_prefix ('-' + str(i) + '_')\n",
    "            data_suc = sel_col.shift(-i).add_prefix ('+' + str(i) + '_')\n",
    "            data_ord = pd.concat([data_ord, data_pre, data_suc], axis=1, sort=False)\n",
    "\n",
    "    df.columns = df.columns.astype(str)\n",
    "    \n",
    "    emd_col = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
    "'23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65',\n",
    "'66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162',\n",
    "'163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291',\n",
    "'292', '293', '294', '295', '296', '297', '298','299']\n",
    "\n",
    "    \n",
    "    #create window for word embedding\n",
    "    window_size_emb = 3\n",
    "    sel_col = df.loc[:,emd_col]\n",
    "    data_ord = pd.concat([data_ord, sel_col], axis=1, sort=False)\n",
    "    for i in range (1, math.ceil(window_size_emb/2)):\n",
    "        data_pre = sel_col.shift(i).add_prefix ('-' + str(i) + '_')\n",
    "        data_suc = sel_col.shift(-i).add_prefix ('+' + str(i) + '_')\n",
    "        data_ord = pd.concat([data_ord, data_pre, data_suc], axis=1, sort=False)\n",
    "        \n",
    "    return data_ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = word_embedding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sliding_window(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('04_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
